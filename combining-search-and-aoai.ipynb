{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install ./azure_search_documents-11.4.0b12-py3-none-any.whl --quiet --force-reinstall\n",
    "! pip install openai python-dotenv --quiet --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy-v3-index\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient  \n",
    "from azure.search.documents.models import (\n",
    "    QueryAnswerType,\n",
    "    QueryCaptionType,\n",
    "    QueryLanguage,\n",
    "    QueryType,\n",
    "    VectorizableTextQuery,\n",
    ")\n",
    "from dotenv import load_dotenv  \n",
    "import os  \n",
    "\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from openai import AzureOpenAI\n",
    "  \n",
    "load_dotenv()  \n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")  \n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")  \n",
    "model: str = \"text-embedding-ada-002\"  \n",
    "default_credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which is more comprehensive, Northwind Health Plus vs Northwind Standard?\"  \n",
    "\n",
    "search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=default_credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pure Vector Search\n",
    "\n",
    "vector_query = VectorizableTextQuery(text=query, k=1, fields=\"vector\", exhaustive=True)\n",
    "# Use the below query to pass in the raw vector query instead of the query vectorization\n",
    "# vector_query = RawVectorQuery(vector=generate_embeddings(query), k=3, fields=\"vector\")\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"parent_id\", \"chunk_id\", \"chunk\"],\n",
    "    top=1\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"parent_id: {result['parent_id']}\")  \n",
    "    print(f\"chunk_id: {result['chunk_id']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['chunk']}\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Search\n",
    "vector_query = VectorizableTextQuery(text=query, k=1, fields=\"vector\", exhaustive=True)\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=query,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"parent_id\", \"chunk_id\", \"chunk\"],\n",
    "    top=1\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"parent_id: {result['parent_id']}\")  \n",
    "    print(f\"chunk_id: {result['chunk_id']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"Content: {result['chunk']}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Hybrid Search\n",
    "vector_query = VectorizableTextQuery(text=query, k=2, fields=\"vector\", exhaustive=True)\n",
    "\n",
    "results = search_client.search(  \n",
    "    search_text=query,\n",
    "    vector_queries=[vector_query],\n",
    "    select=[\"parent_id\", \"chunk_id\", \"chunk\"],\n",
    "    query_type=QueryType.SEMANTIC, query_language=QueryLanguage.EN_US, semantic_configuration_name='my-semantic-config', query_caption=QueryCaptionType.EXTRACTIVE, query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "    top=2\n",
    ")\n",
    "\n",
    "semantic_answers = results.get_answers()\n",
    "for answer in semantic_answers:\n",
    "    if answer.highlights:\n",
    "        print(f\"Semantic Answer: {answer.highlights}\")\n",
    "    else:\n",
    "        print(f\"Semantic Answer: {answer.text}\")\n",
    "    print(f\"Semantic Answer Score: {answer.score}\\n\")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"parent_id: {result['parent_id']}\")  \n",
    "    print(f\"chunk_id: {result['chunk_id']}\")  \n",
    "    print(f\"Reranker Score: {result['@search.reranker_score']}\")\n",
    "    print(f\"Content: {result['chunk']}\")  \n",
    "\n",
    "    captions = result[\"@search.captions\"]\n",
    "    if captions:\n",
    "        caption = captions[0]\n",
    "        if caption.highlights:\n",
    "            print(f\"Caption: {caption.highlights}\\n\")\n",
    "        else:\n",
    "            print(f\"Caption: {caption.text}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can play around with mixing prompt engineering and the different search options to see offers the best results for your use case. Other things to consider would be what other cog skills you might want to employ and then persist information to the index e.g. sentiment analysis. Besides that you can also consider custom skills either to enhance auth options available to you or to run any special coding. Among the cog skills is integration with Azure Machine Learning so you can perform traditional machine learning to generate analysis.\n",
    "\n",
    "https://github.com/openai/openai-python/blob/main/examples/azure_ad.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'code': 'PermissionDenied', 'message': 'Principal does not have access to API/Operation.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Projects\\aoai-search-vector\\combining-search-and-aoai.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/aoai-search-vector/combining-search-and-aoai.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m endpoint \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mgetenv(\u001b[39m\"\u001b[39m\u001b[39mAZURE_OPENAI_ENDPOINT\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/aoai-search-vector/combining-search-and-aoai.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m client \u001b[39m=\u001b[39m AzureOpenAI(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/aoai-search-vector/combining-search-and-aoai.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     api_version\u001b[39m=\u001b[39mapi_version,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/aoai-search-vector/combining-search-and-aoai.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     azure_endpoint\u001b[39m=\u001b[39mendpoint,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/aoai-search-vector/combining-search-and-aoai.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     azure_ad_token_provider\u001b[39m=\u001b[39mtoken_provider,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/aoai-search-vector/combining-search-and-aoai.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Projects/aoai-search-vector/combining-search-and-aoai.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m completion \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/aoai-search-vector/combining-search-and-aoai.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-35-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m,  \u001b[39m# e.g. gpt-35-instant\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/aoai-search-vector/combining-search-and-aoai.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     messages\u001b[39m=\u001b[39;49m[\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/aoai-search-vector/combining-search-and-aoai.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         {\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/aoai-search-vector/combining-search-and-aoai.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/aoai-search-vector/combining-search-and-aoai.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mHow do I output all files in a directory using Python?\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/aoai-search-vector/combining-search-and-aoai.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         },\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/aoai-search-vector/combining-search-and-aoai.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     ],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/aoai-search-vector/combining-search-and-aoai.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/aoai-search-vector/combining-search-and-aoai.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mprint\u001b[39m(completion\u001b[39m.\u001b[39mmodel_dump_json(indent\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m))\n",
      "File \u001b[1;32mc:\\Projects\\aoai-search-vector\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:301\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    299\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    300\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 301\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Projects\\aoai-search-vector\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:598\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    552\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    553\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    596\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    597\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 598\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[0;32m    599\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    600\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[0;32m    601\u001b[0m             {\n\u001b[0;32m    602\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[0;32m    603\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[0;32m    604\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[0;32m    605\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[0;32m    606\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[0;32m    607\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[0;32m    608\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[0;32m    609\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[0;32m    610\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[0;32m    611\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[0;32m    612\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[0;32m    613\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[0;32m    614\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[0;32m    615\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[0;32m    616\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[0;32m    617\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[0;32m    618\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[0;32m    619\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[0;32m    620\u001b[0m             },\n\u001b[0;32m    621\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[0;32m    622\u001b[0m         ),\n\u001b[0;32m    623\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[0;32m    624\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[0;32m    625\u001b[0m         ),\n\u001b[0;32m    626\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[0;32m    627\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    628\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[0;32m    629\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Projects\\aoai-search-vector\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1096\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1082\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[0;32m   1083\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1084\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1091\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1092\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m   1093\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[0;32m   1094\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[0;32m   1095\u001b[0m     )\n\u001b[1;32m-> 1096\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[1;32mc:\\Projects\\aoai-search-vector\\.venv\\Lib\\site-packages\\openai\\_base_client.py:856\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    847\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    848\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    849\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    854\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    855\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m--> 856\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[0;32m    857\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[0;32m    858\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[0;32m    859\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    860\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[0;32m    861\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[0;32m    862\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Projects\\aoai-search-vector\\.venv\\Lib\\site-packages\\openai\\_base_client.py:908\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    905\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n\u001b[0;32m    906\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[1;32m--> 908\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    909\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    910\u001b[0m     \u001b[39mif\u001b[39;00m response \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'code': 'PermissionDenied', 'message': 'Principal does not have access to API/Operation.'}}"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "\n",
    "# may change in the future\n",
    "# https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n",
    "api_version = \"2023-07-01-preview\"\n",
    "\n",
    "# https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    azure_ad_token_provider=token_provider,\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-35-turbo\",  # e.g. gpt-35-instant\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How do I output all files in a directory using Python?\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(completion.model_dump_json(indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
